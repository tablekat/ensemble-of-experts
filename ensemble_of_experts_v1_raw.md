I will develop a publishable academic paper outlining the Ensemble of Experts (EoE) architecture, formalizing its design, distinguishing it from existing architectures such as Hierarchical Mixture of Experts (HME), Nested Mixture of Experts (NMOE), and Google's Sparsely-Gated Mixture-of-Experts. The paper will discuss its novelty, theoretical motivations, computational feasibility, and potential applications.

Additionally, I will suggest an experimental setup to empirically evaluate EoE against existing MoE architectures. Figures and diagrams will be included to illustrate the structure and behavior of the model. I will let you know when the document is ready.

# Ensemble of Experts (EoE) Architecture: An Extension of MoE and AoE

## Introduction 
Ensemble of Experts (EoE) is a novel neural network architecture that extends the Mixture-of-Experts (MoE) paradigm with a **hierarchical ensemble** of expert layers. The high-level motivation behind EoE is to combine inspirations from deep neural networks, human cognitive processes, and network routing optimization to create a more **specialized and efficient model**. In standard MoE, a single layer of experts is used with a routing mechanism to select a few experts per input ([Autonomy-of-Experts Models](https://arxiv.org/html/2501.13074v1#:~:text=MoE%20in%20LLMs%20involves%20dividing,expert%20knowledge%20enhances%20downstream%20performance))  By contrast, EoE **stacks multiple layers of expert subnetworks** in a grid-like arrangement, allowing a sequence of expert decisions reminiscent of layered processing in the brain. This design enables *coarse-to-fine specialization*: early expert layers make high-level routing decisions (analogous to broad cognitive choices), and later layers focus on finer-grained processing. 

EoE is inspired in part by how human cognition tackles complex tasks via **multiple specialized modules** working in sequence. For example, when solving a problem, humans might first identify the category of the problem (high-level decision) and then apply a specific method or skill (detailed processing). Similarly, EoE’s first expert layer could identify an input’s general domain or modality, and subsequent layers apply domain-specific experts. This layered expert strategy also takes cues from **network optimization**, where tasks are routed through a series of nodes, each optimizing a part of the route. By breaking a complex decision into smaller routing decisions across layers, EoE aims to achieve better overall routing of information through the network.

In summary, the proposed EoE architecture introduces *hierarchical expert layers* with ensemble decision-making at each layer. We hypothesize that this yields three main benefits: (1) **Improved specialization** – each expert can focus on a narrower task subspace defined by previous layer decisions; (2) **Efficient computation** – by activating only a sparse subset of experts per layer, the model uses far fewer computations than a dense network of equivalent size; and (3) **Flexible multimodal processing** – different expert pathways can be devoted to different modalities or subtasks, enabling a single EoE model to handle diverse inputs. **Figure 1** provides a conceptual overview of the EoE architecture, showing how an input passes through multiple expert layers and how only certain experts “fire” at each layer. In the following sections, we formally define the EoE architecture and discuss how it relates to prior work, its theoretical properties, and experimental considerations.

## Related Work 
**Mixture-of-Experts (MoE):** The MoE framework was originally introduced in the 1990s and formulates a network as a gated combination of multiple expert models. In a classic MoE, a *gating network* learns to partition the input space and assign each input to one or a few expert networks. Jacobs et al. and Jordan & Jacobs demonstrated early forms of MoE and hierarchical gating, using expectation-maximization to train such systems. In a *flat* (single-layer) MoE, all experts receive the same input, and a router or gating function chooses which expert’s output (or outputs) to use. Modern MoEs, such as those used in large language models, divide a large feed-forward network into many smaller expert networks and use a learned router to activate a subset of experts for each input ([Autonomy-of-Experts Models](https://arxiv.org/html/2501.13074v1#:~:text=MoE%20in%20LLMs%20involves%20dividing,expert%20knowledge%20enhances%20downstream%20performance))  This *sparse activation* means only a few experts’ parameters are used per input, making MoEs very parameter-efficient (able to scale to billions of parameters without proportional compute cost) ([[1701.06538] Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer](https://arxiv.org/abs/1701.06538#:~:text=efficiency%20on%20modern%20GPU%20clusters,critical%20for%20absorbing%20the%20vast))  For example, Shazeer et al. (2017) introduced a sparsely-gated MoE layer with *thousands* of experts, where a trainable gating network selects a small combination of experts for each example ([[1701.06538] Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer](https://arxiv.org/abs/1701.06538#:~:text=efficiency%20on%20modern%20GPU%20clusters,critical%20for%20absorbing%20the%20vast))  Google’s recent MoE-based models (e.g. GShard and Switch Transformer) showed that such architectures can dramatically increase model capacity while maintaining similar computational cost by activating only ~2 experts out of e.g. 256 for each input ([[1701.06538] Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer](https://arxiv.org/abs/1701.06538#:~:text=improvements%20in%20model%20capacity%20with,137%20billion%20parameters%20is%20applied)) 

**Autonomy-of-Experts (AoE):** A more recent development in MoE research is the Autonomy-of-Experts model by Lv et al. (2024), which addresses a key limitation of traditional MoEs: the separation of the router’s decision from the experts’ knowledge ([Autonomy-of-Experts Models](https://arxiv.org/html/2501.13074v1#:~:text=Mixture,In%20AoE%2C%20routers))  ([Autonomy-of-Experts Models](https://arxiv.org/html/2501.13074v1#:~:text=awareness%20reflected%20in%20the%20scale,computing))  In a standard MoE, the router makes a “best guess” of which expert is appropriate, without direct feedback from the experts themselves, potentially leading to suboptimal choices. AoE proposes to **eliminate the central router** and let experts self-determine their involvement. In an AoE layer, *all experts* initially process the input in a lightweight manner to estimate their own ability to handle it (for instance, by computing the norm of their internal activation) ([Autonomy-of-Experts Models](https://arxiv.org/html/2501.13074v1#:~:text=autonomously%20select%20themselves%20to%20process,ranking%20experts%20proceed))  The experts then effectively *vote* on their relevance: only the top-$k$ experts with the highest activation norms continue to fully process the input, while the others abort further computation ([Autonomy-of-Experts Models](https://arxiv.org/html/2501.13074v1#:~:text=awareness%20reflected%20in%20the%20scale,computing))  This autonomous gating means each expert “knows what it knows” and can opt in or out. AoE was shown to improve expert selection and downstream performance over traditional MoE in language modeling tasks ([Autonomy-of-Experts Models](https://arxiv.org/html/2501.13074v1#:~:text=are%20ranked%20based%20on%20their,MoE%20models%20with%20comparable%20efficiency))  The EoE architecture takes inspiration from AoE by considering expert-informed routing decisions at each layer (as opposed to relying purely on an external routing network).

**Hierarchical Mixture-of-Experts (HME):** Jordan and Jacobs’ Hierarchical MoE is an earlier architecture that organizes experts in a tree-structured hierarchy ([](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=78738cf6d9f3d8154d3f3e41484337f837e858a6#:~:text=The%20hierarchical%20mixture,an%20output%20vector%20%16ij%20for))  In HME, gating networks are arranged in a binary tree (or multi-branch tree) such that an input first encounters a top-level gating which selects a branch, then a second-level gating for a sub-branch, and so on, until reaching a leaf expert. Each internal node gating produces a probability distribution over its child experts (partitioning unity among them) ([](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=78738cf6d9f3d8154d3f3e41484337f837e858a6#:~:text=The%20hierarchical%20mixture,The%20expert))  The leaf nodes are expert models that produce the final output; the output is usually a weighted combination of leaf expert outputs, where the weights are the product of gating probabilities along the path ([](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=78738cf6d9f3d8154d3f3e41484337f837e858a6#:~:text=The%20hierarchical%20mixture,an%20output%20vector%20%16ij%20for))  This approach allows a form of coarse-to-fine routing: the top of the tree makes a broad decision, and deeper in the tree more refined decisions are made. HME effectively *nests MoE decisions in a hierarchy*, which is conceptually similar to EoE’s multi-layer expert structure. However, a classic HME uses a rigid tree (often binary splits at each node), whereas EoE generalizes this to an $M \times N$ grid of experts (not necessarily a strict binary tree). **Figure 2(a)** illustrates a simple HME: a binary tree of experts with gating at each split. One limitation of HME is that its structure (tree topology) must be decided a priori, and training often relied on specialized EM algorithms ([](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=78738cf6d9f3d8154d3f3e41484337f837e858a6#:~:text=The%20hierarchical%20mixture,an%20output%20vector%20%16ij%20for)) 

**Nested Mixture-of-Experts (NMoE):** More recently, Ahn and Sentis (2021) proposed the Nested MoE architecture, motivated by modeling hybrid dynamical systems ([[2011.10605] Nested Mixture of Experts: Cooperative and Competitive Learning of Hybrid Dynamical System](https://ar5iv.org/pdf/2011.10605#:~:text=As%20depicted%20in%20Figure%C2%A02%2C%20an,computes%20an%20output%20as%20follows))  ([[2011.10605] Nested Mixture of Experts: Cooperative and Competitive Learning of Hybrid Dynamical System](https://ar5iv.org/pdf/2011.10605#:~:text=,on%20it%20are%20cooperative%20MOEs))  NMoE introduces the idea of experts that are themselves MoE networks. In their design, the *top layer* is a “competitive” MoE: a gating network chooses one among several top-level experts for each input (similar to a standard MoE gating) ([[2011.10605] Nested Mixture of Experts: Cooperative and Competitive Learning of Hybrid Dynamical System](https://ar5iv.org/pdf/2011.10605#:~:text=As%20depicted%20in%20Figure%C2%A02%2C%20an,computes%20an%20output%20as%20follows))  Crucially, each top-level expert is not a monolithic network but a *cooperative MoE* that contains two sub-experts and its own gating mechanism ([[2011.10605] Nested Mixture of Experts: Cooperative and Competitive Learning of Hybrid Dynamical System](https://ar5iv.org/pdf/2011.10605#:~:text=As%20depicted%20in%20Figure%C2%A02%2C%20an,computes%20an%20output%20as%20follows))  In other words, each expert at the first level internally mixes two specialist models (for example, a physics-based model and a neural network model, in the case of hybrid systems). The gating within a cooperative expert blends its two sub-experts’ outputs, rather than picking one exclusively. The phrase *nested* reflects that an MoE is nested inside another MoE. Figure 2(b) conceptually illustrates NMoE: the first layer gating (pink) picks one expert box, and inside that box a secondary gating (green) blends two sub-experts. NMoE was shown to effectively incorporate prior knowledge (e.g. known physics equations and learned residuals) by assigning them to different sub-experts that cooperate ([[2011.10605] Nested Mixture of Experts: Cooperative and Competitive Learning of Hybrid Dynamical System](https://ar5iv.org/pdf/2011.10605#:~:text=so%20that%20the%20local%20experts,the%20input%20to%20a%20single))  ([[2011.10605] Nested Mixture of Experts: Cooperative and Competitive Learning of Hybrid Dynamical System](https://ar5iv.org/pdf/2011.10605#:~:text=The%20second%20type%20of%20prior,box%20model%29%20as%20local))  In comparison, EoE generalizes the notion of nested experts to potentially many layers: rather than just a two-level nest, EoE can be seen as *multiple levels of gating and experts*, which could emulate an NMoE (if N=2 layers) or an HME (if structured as a tree).

**Sparsely-Gated and Multi-gated MoEs:** The success of sparsely-gated MoE in large-scale models (e.g. GShard (Lepikhin et al., 2021) and Switch Transformer (Fedus et al., 2022)) has demonstrated the scalability of expert-based models. Switch Transformer in particular simplified MoE by routing each input token to **exactly one expert** (top-1 routing) to reduce communication overhead, combined with a load-balancing loss to ensure all experts get utilized ([Autonomy-of-Experts Models](https://arxiv.org/html/2501.13074v1#:~:text=,Black%2C%20S))  ([Autonomy-of-Experts Models](https://arxiv.org/html/2501.13074v1#:~:text=,URL))  While this approach is efficient, it forgoes the combination of multiple experts per token. EoE can be configured to use a similar sparse gating at each layer (e.g., picking the single best expert per layer, yielding a single path through the network), or to allow a small ensemble of experts at the final layer. Another related idea is the *multi-gated MoE* (used in some multilingual models), where separate gating networks route different features or groups of inputs to experts. EoE’s architecture, with its grid of experts, could naturally accommodate *multiple gating criteria per layer* (for instance, one gating head could route by language, another by topic, in the first layer), though exploring that is beyond the scope of this paper.

In summary, EoE builds upon concepts from these prior works. Like HME and NMoE, it introduces **hierarchical structure** into expert routing, and like AoE, it emphasizes **expert-driven routing decisions**. The next section formalizes the EoE architecture, highlighting how it structurally differs from a standard one-layer MoE and from these related designs (**Figure 2** provides a visual comparison of architectures for reference).

## Formalization of the EoE Architecture 
**Overview:** The Ensemble of Experts (EoE) architecture is organized as an $M \times N$ grid of expert units, arranged in $N$ sequential **layers** (rows) with $M$ **experts per layer** (columns). Figure 1 depicts an example EoE configuration with $N=3$ layers and $M=4$ experts in each layer (for illustration). Each expert is a specialized neural network (e.g. an MLP or convolutional block) focusing on a particular feature space or sub-task. The layers are connected by a **hierarchical activation mechanism**: the pattern of which expert(s) become active in layer $\ell$ influences which experts are considered in layer $\ell+1$. Formally, we can denote the experts as $E_{\ell,i}$ where $\ell \in \{1,\dots,N\}$ is the layer index and $i \in \{1,\dots,M\}$ indexes the expert within that layer. 

**Input Propagation:** An input sample (e.g. a data instance or a token in an LLM) first enters the first layer ($\ell=1$), which contains $M$ candidate experts $\{E_{1,1}, E_{1,2}, \dots, E_{1,M}\}$. A gating function $G_1$ (or an AoE-style expert self-selection process) determines which expert(s) in layer 1 will actively process the input. This could be implemented as a softmax-based router that assigns a probability $p_{1,i}$ to each expert $E_{1,i}$ (with $\sum_i p_{1,i}=1$), or as an autonomous gating where each expert evaluates itself and the top $k$ are chosen. For simplicity, assume a router that selects a subset $A_1 \subseteq \{1,\dots,M\}$ of experts to activate (often $|A_1|=k \ll M$, e.g. $k=1$ or $2$ for sparsity). The selected experts in layer 1 each process the input and produce some intermediate output (or representation). Experts not selected remain inactive (skipped) for this input, saving computation.

**Hierarchical Routing Between Layers:** The core innovation of EoE is that the choice of experts in layer 1 will constrain and inform the choice of experts in layer 2, and so on. Instead of every expert in layer 2 seeing the combined output of layer 1, we define **directed connections** between specific experts in layer 1 and specific experts in layer 2. In general, each expert $E_{\ell,i}$ in layer $\ell$ is **connected to** a subset of experts $\{E_{\ell+1, j} : j \in \mathcal{C}_{\ell,i}\}$ in the next layer (where $\mathcal{C}_{\ell,i}\subseteq \{1,\dots,M\}$ is the connectivity set of indices). This connectivity can be designed to impose a hierarchy. For example, layer 1 might have broad experts whose outputs feed into only a related subgroup of experts in layer 2. If $E_{1,2}$ is activated for an input, then the gating $G_2$ for layer 2 will be applied **only over its connected experts** $\{E_{2,j}: j \in \mathcal{C}_{1,2}\}$, rather than all $M$ experts. In effect, the activation from layer 1 *routes the input into a specific region of the layer 2 expert space*. This *conditional routing* can be viewed as a multi-stage decision: first choose an expert in layer 1, then given that choice, choose an expert in layer 2, etc. If we unroll the full sequence, a path through the network might look like $(E_{1,i_1} \rightarrow E_{2,i_2} \rightarrow \cdots \rightarrow E_{N,i_N})$, where each $i_{\ell+1} \in \mathcal{C}_{\ell, i_\ell}$ depends on the previous expert. This sequence of experts forms an ensemble in the sense that each contributes at a different stage, and together they determine the final outcome.

Mathematically, the output of an EoE can be formalized as follows. Let $h_\ell$ denote the hidden representation after layer $\ell$ is applied. We have $h_0 = x$ as the input. At layer $\ell=1$, 
\[ h_1 = \sum_{i=1}^M p_{1,i} \, E_{1,i}(h_0), \] 
where $E_{\ell,i}(h)$ denotes the output of expert $i$ at layer $\ell$ given input $h$, and $p_{1,i}$ are gating weights (which are non-zero only for the selected expert(s)). If the gating selects a single expert (say $i_1$), this simplifies to $h_1 = E_{1,i_1}(x)$. Now for layer 2, the gating $G_2$ produces weights $p_{2,j}$ **conditional on the active expert from layer 1**. One way to imagine this is to have a separate gating network for each possible expert in layer 1, i.e. $G_2^{(i_1)}$ outputs probabilities for experts in $\mathcal{C}_{1,i_1}$. Then 
\[ h_2 = \sum_{j \in \mathcal{C}_{1,i_1}} p_{2,j}^{(i_1)} \, E_{2,j}(h_1). \] 
If only one expert $E_{2,i_2}$ is chosen, $h_2 = E_{2,i_2}(h_1)$. This process continues through layer $N$. At the final layer $N$, we obtain outputs $o_{i_N} = E_{N,i_N}(h_{N-1})$ from the selected expert(s) in the last layer. The **final output** of the EoE model is then an ensemble of the active final-layer experts' outputs. In many cases, a single expert might be chosen in the last layer, in which case the model’s output is simply that expert’s output. In a more general setting, one could allow multiple final experts and combine their outputs (e.g. by weighted sum or another fusion mechanism), truly forming an ensemble output. The architecture is flexible: by adjusting the number of active experts per layer and the combination method, EoE can emulate purely *mixture* behavior (one expert per layer, like a chain) or a *stacked ensemble* (multiple experts contributing to the final result).

**Final Layer Role:** The final row (layer $N$) of experts is responsible for producing the task-specific output (e.g. class probabilities, next-word prediction logits, regression value, etc.). Importantly, **only the experts in the final layer actually output to the external task** – earlier layers produce intermediate representations. This is analogous to how in a deep neural network, only the last layer produces the final predictions, with preceding layers feeding into it. In EoE, the last-layer experts can be seen as *leaf experts* (similar to leaves in an HME tree) that specialize in the most fine-grained distinctions required for the task. For instance, in a language model EoE, final-layer experts might specialize in specific linguistic phenomena or contexts (having been routed through previous layers that partition by topic or syntax). The output of the model is typically a weighted sum of the active final experts’ outputs. If the gating decisions are *hard* (one expert per layer), then effectively the final output is just from one expert path. If the gating is *soft* (multiple experts per layer with weights), the final output might combine several experts’ contributions, which can improve robustness at the cost of more computation.

**Nested and Ensemble Structures:** Notably, EoE’s formulation allows *recursive nesting*. Each expert $E_{\ell,i}$ itself could internally be another EoE or MoE. This is a “Mixture of Mixture-of-Experts” concept alluded to earlier. In practice, this could mean an expert in the grid is actually a small MoE specialized to some sub-problem. EoE does not forbid this—in fact, a hierarchical EoE with two layers and appropriate connectivity could reproduce the NMoE design (top layer picks an expert which inside has two sub-experts blended). More generally, EoE could be used to build a very deep expert hierarchy: e.g. a binary-tree HME of depth 3 can be mapped onto an EoE with $N=3$ layers and $M=2$ experts per layer (and connectivity such that each expert only connects to a unique child in the next layer, forming a chain). Because of this generality, EoE can be seen as a *unifying framework* that includes flat MoE, HME, and NMoE as special cases. The term "Ensemble" in EoE also highlights that at the final decision, multiple experts (from different layers) have influenced the outcome, and potentially multiple final experts could be combined, forming an ensemble output rather than relying on a single expert’s output.

In **Figure 1**, we illustrate a simple EoE with a 3×3 grid of experts. The figure shows how an input $x$ is first routed to one of the 3 experts in layer 1 (expert 1 in the example). Then, based on that choice, the input is passed to a particular subset of experts in layer 2 (experts 2 and 3 are possible, but expert 2 gets the highest gating weight and is selected). Finally, in layer 3, one expert (expert 3) is chosen and produces the output $o$. The figure contrasts this with **Figure 2**, where (a) shows a traditional MoE (one layer, many experts all considered in parallel), and (b) shows a two-level nested MoE (similar to NMoE). These conceptual diagrams highlight that EoE uses *both multiple layers and multiple experts per layer*, whereas prior architectures typically vary along one dimension (either depth as in HME/NMoE or width as in MoE). 

## Theoretical Considerations 
In this section, we discuss the computational efficiency, potential benefits, and trade-offs of the EoE architecture. We focus on how the **sparsity** of activation is maintained (or even enhanced) in EoE, how experts might specialize (especially in multimodal settings), and strategies to optimize computation by skipping low-relevance experts.

**Computational Efficiency:** One might wonder about the cost of stacking multiple expert layers versus a single-layer MoE. Interestingly, EoE can preserve the *sparse activation* principle at each layer, so the overall activated fraction of parameters remains low. For instance, suppose a standard MoE selects $k$ out of $M$ experts. In an EoE with $N$ layers, if we also select $k$ out of $M$ at each layer, the total number of experts evaluated per input is $N \cdot k$ (one per layer, assuming $k=1$ for simplicity of analysis). Although $N$ times more expert blocks are processed sequentially, each expert is smaller than a giant flat MoE expert (since the total parameters are distributed across layers) and the gating at each layer is dealing with fewer candidates (only $M$ per layer, rather than $M \cdot N$ all at once). Moreover, because of the **hierarchical constraint**, later layers only consider a subset of experts relevant to the earlier decisions, potentially reducing wasted computation on irrelevant experts. If the connectivity is designed such that each layer partitions the input space, an input will *not even be considered* by many experts in deeper layers (those not connected to the active path). This is an advantage over a flat MoE where *every* expert is at least considered by the router (even if not ultimately used, the router must compute scores for all). In EoE, the routing computation is also hierarchical: the router (or experts in AoE style) at layer $\ell$ only scores the experts in that layer, not all experts globally. Thus, the *routing computation* per layer is cheaper, and if $N$ is not too large, the overall routing cost can be comparable to a flat MoE’s single large router. 

There is a trade-off in latency versus throughput: A flat MoE can evaluate experts in parallel since all experts operate on the same input concurrently (then discard all but the top ones), whereas EoE’s layers are sequential (cannot compute layer 2 before layer 1’s choice is made). This sequential nature might increase latency (the critical path is longer). However, each expert layer is doing less work and could be faster, and if $N$ is small (a shallow hierarchy), the latency hit is minor. In scenarios where parallel processing is abundant (e.g. on a GPU or TPU), one could even parallelize across *multiple inputs* such that while one input is in layer 2, another input is in layer 1, etc., to keep utilization high (pipeline parallelism). Overall, EoE trades a bit of increased inference depth for a potential exponential increase in model capacity (since $M$ experts in $N$ layers can represent up to $M^N$ distinct expert-path combinations).

**Sparsity and Skipping Low-Weight Experts:** Similar to sparsely-gated MoEs, EoE ensures that only a subset of experts are active for each input. If the gating network produces a very low weight for an expert (or an expert’s self-activation norm is low in an AoE setting), that expert’s computation can be skipped entirely. In fact, EoE provides multiple opportunities to cull unnecessary computation: at each layer, only the top experts continue. This cascading sparsity means that an input quickly *funnels down* to a small set of possible paths. For example, at layer 1, maybe 2 experts are activated; for each of those, at layer 2 perhaps 2 are activated, but only the branch of the first expert is followed, etc. By layer N, you might end up with just one active expert. From a computational graph perspective, EoE behaves like a tree of conditional execution: many branches exist, but only one branch (or a few) is taken for any given input. This is highly efficient if implemented with conditional computation frameworks. One challenge is that typical hardware (GPUs) do best with static, data-parallel computation. Techniques such as grouping inputs that take the same path can help amortize the cost (this was explored in MoE literature with techniques to improve load balance). If an expert’s gate probability is below a threshold, one can force it to zero (“hard” gating) to avoid wasteful small computations – this is analogous to *Top-K gating* where only the top $K$ experts’ weights are kept and others set to 0 ([[1701.06538] Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer](https://arxiv.org/abs/1701.06538#:~:text=efficiency%20on%20modern%20GPU%20clusters,critical%20for%20absorbing%20the%20vast))  EoE naturally aligns with Top-1 or Top-2 gating at each layer to keep the computation sparse. Additionally, if certain entire *paths* (combination of experts) end up rarely used, they could be pruned from the model to simplify it – although care must be taken not to prematurely remove potentially useful experts (this ventures into *dynamic model pruning* territory).

**Specialization of Experts (Modularity):** By design, EoE encourages experts to become specialists, **especially across modalities or high-level categories**. One can assign different intended roles to different columns of the grid. For example, in a multimodal EoE model, layer 1 experts might each specialize in a particular modality: one expert for vision inputs, one for language, one for audio, etc. The gating in layer 1 would route inputs to the appropriate modality expert. Then, layer 2 might further specialize within that modality (say, for vision: one expert for faces, one for landscapes, one for text in images, etc.). In this way, EoE could implement a modality hierarchy. Indeed, the architecture is well-suited to **multi-task and multi-modal learning**, as it can allocate entire sub-networks (subsets of experts) to different domains, while still allowing sharing of knowledge at certain layers if beneficial. The ensemble aspect means if an input requires cross-modal reasoning, the model could potentially activate a vision expert and a language expert in the first layer (if gating allows multiple), and merge their information in subsequent layers. This is more flexible than having completely separate models per modality. Another specialization use-case is for different *skills or knowledge bases* in a large system (analogous to how the human brain has specialized regions). For instance, an EoE-based assistant AI might have one expert pathway for mathematical reasoning, another for commonsense reasoning, and another for factual recall. A question requiring math would be routed by layer 1 to the math expert, and then perhaps layer 2 chooses a specific type of math problem expert, etc., yielding the final answer from a very targeted expert. This kind of targeted specialization can improve both accuracy and interpretability (we can identify which expert handled the query). 

**Capacity vs. Utilization Trade-off:** EoE, like other MoE variants, offers enormous parameter capacity, but ensuring those parameters are effectively used is a challenge. With multiple layers of experts, there is a risk that some experts (especially in deeper layers) might rarely be activated if the gating decisions in earlier layers seldom route inputs to them. This is akin to part of the network being “dead” for most inputs. The trade-off is between having many experts (high capacity) and making sure each has a slice of the input distribution to specialize on. Prior MoE works addressed this with load-balancing losses or random routing to give experts learning opportunities ([Autonomy-of-Experts Models](https://arxiv.org/html/2501.13074v1#:~:text=experts%C2%A0%28Shazeer%20et%C2%A0al,2021%3B%20Clark%20et%C2%A0al))  In EoE, similar techniques could be applied at each layer: e.g., add a loss term that encourages each expert in layer $\ell$ to get a minimum fraction of the routing probability mass across the training data. Another approach, inspired by AoE, is to let experts self-advertise when they are confident to handle an input – this could naturally balance load because an expert will not compete for inputs it’s not well-trained on. The hierarchical nature might actually *help utilization*: since each layer’s gating only competes among a smaller group of experts (those in that layer or connected group), there’s less chance of one expert completely shadowing all others globally. Each layer can develop a roughly even division of labor on its portion of the decision tree.

**Theoretical Benefits:** Summarizing the benefits:
- *Exponential Representation Power:* An EoE with $N$ layers of $M$ experts each can represent up to $M^N$ combinations of expert pathways. In theory, this allows a very finely partitioned input space (if each path corresponds to a region). Even if each expert is a simple function, the combination can represent a highly complex overall function. This is analogous to how a deep decision tree can carve the input space into many regions.
- *Coarse-to-Fine Decision Making:* The hierarchical gating means decisions are made in stages, which may be easier to learn. The first layer can learn broad distinctions without being distracted by minutiae, because it knows further layers will handle those. This divide-and-conquer strategy could lead to faster training convergence compared to a single huge flat gating trying to learn everything at once.
- *Interpretable Routing:* There is a potential interpretability gain: one can inspect which expert was chosen at each layer for a given input, yielding a reasoning trace (“Input went to expert (Topic: Sports) at layer1, then expert (Question type: factual) at layer2, then expert (Output format: short answer) at layer3, for example). This is reminiscent of how humans might break down reasoning steps, and can help debug or trust the model’s decisions. Standard MoE also allows some interpretation (which expert was used), but EoE provides a *multi-step trace*.

**Trade-offs and Potential Drawbacks:** 
- *Complexity:* The architecture is more complex than a standard neural network, with many conditional branches. This can complicate implementation and require careful engineering (as discussed in the next section). 
- *Training Difficulty:* More layers of gating mean more points where errors or uncertainties in routing can occur. If the first layer routes an input incorrectly, none of the later experts (no matter how good) can fix that because the input went down the wrong path. This places importance on robust training of early layers. Techniques like curriculum learning (first train gating to roughly partition the data, then fine-tune deeper layers) or joint training with regularization might be needed.
- *Sequential Computation:* As noted, inference requires sequentially activating layers. For tasks requiring very fast real-time response, a deep EoE might be slower than a shallower model. There is a sweet spot in choosing $N$ (the number of layers) based on task complexity and latency requirements.
- *Parameter Overhead:* While each input only uses a few experts, the total number of parameters (if $M$ and $N$ are large) could be huge. This is memory-intensive and could be inefficient if many experts are rarely used. One has to ensure that the model size is justified by the task (for example, EoE may shine in massive multi-domain models, but for a small single-task model it might be overkill).
- *Edge Cases:* If an input somehow doesn’t clearly belong to any expert’s domain at layer1 (say the gating outputs are almost uniform), the model might not route it confidently. One can always fall back to combining multiple experts or using a default expert in such cases, but this behaviour needs to be defined. In critical applications, having a “backup” dense path might be considered (though that loses the efficiency benefit).

In theory, EoE stands to significantly improve the **parameter efficiency and specialization** of neural networks by marrying the strengths of deep models and MoE. The next section outlines how we propose to empirically validate these theoretical advantages through experiments.

## Experimental Setup 
To evaluate the Ensemble of Experts architecture, we propose a comprehensive benchmarking experiment comparing EoE against existing MoE-based architectures. The goal of the experiments would be to answer: *Does EoE achieve better performance or efficiency than a conventional MoE (or AoE) for equivalent model sizes?* And *in what scenarios does the multi-layer expert structure prove most beneficial?* We outline the experimental setup in terms of datasets, baseline models, and evaluation metrics.

**Datasets and Tasks:** We suggest evaluating EoE on a diverse set of tasks covering **natural language processing (NLP)**, **computer vision**, and **multimodal** problems to test the architecture’s versatility. For example:
- *Language Modeling (NLP):* Use a standard language modeling benchmark (such as WikiText-103 or the Pile) where large MoE models have shown gains. This will test if EoE can improve perplexity and efficiency for generative modeling of text.
- *Machine Translation (NLP):* Multi-lingual translation tasks (e.g. English to/from French, German, Chinese, etc., using WMT datasets). EoE’s experts could specialize in languages or language families, mimicking a tree of languages (which has known hierarchies) – a good test for expert specialization.
- *Image Classification (Vision):* Use ImageNet-1k or a variant as a vision task. One can insert an EoE layer or backbone in a CNN. For instance, an EoE could replace some layers in a ResNet, where the first expert layer might specialize in color vs texture, and later in object parts. Compare to a standard ensemble or a single MoE layer in the network.
- *Multimodal Question Answering:* Datasets like VQA (Visual Question Answering) or WebQA that involve both images and text questions. Here, an EoE can have one branch of experts for image feature processing and another for question (text) processing, merging in later layers. This tests the modality routing capability.
- *Reinforcement Learning environments:* (Optional) The NMoE paper focused on hybrid dynamical systems. We could similarly test EoE on a continuous control task where different experts handle different dynamic regimes (though this is a more specialized scenario).

**Baseline Models for Comparison:** We would compare EoE models to multiple baselines to isolate the effect of the architecture:
- **Dense Model Baseline:** A conventional (non-MoE) model with roughly the *same total number of parameters* as the EoE (counting all experts). This baseline checks if EoE’s sparse utilization can outperform a dense model of comparable size. For example, if EoE has 5B parameters but only 500M active per input, compare to a 5B dense transformer.
- **Flat MoE Baseline:** A single-layer MoE with $M \times N$ experts (so that the total number of experts is equal to EoE’s total experts, but all in one layer). This baseline tests whether having multiple layers is better than having the same number of experts in one layer. We would use state-of-the-art MoE implementations (with Top-1 or Top-2 routing). If available, also include **Switch Transformer** style (which is a special case of MoE with hard routing to one expert).
- **Autonomy-of-Experts Baseline (AoE):** If our EoE uses a standard router, it’s fair to also compare to an AoE version of MoE (single layer AoE as in Lv et al., 2024). This will tell us if improvements come from hierarchical structure or from the gating mechanism. Conversely, we might implement EoE with AoE gating at each layer, which would then be compared to router-based EoE.
- **Hierarchical or Nested MoE Baselines:** If feasible, we can configure an HME-like model (two-layer tree) or the published NMoE model for a specific task (like a two-layer mixture specialized similar to their design) to compare with EoE’s performance. For instance, on a multimodal task, a two-layer HME might manually route by modality then by question type, etc. However, these baselines are not as common in modern toolkits, so this comparison might be more qualitative unless we implement it.
- **Ensemble of Separate Models:** As an additional point of reference, we could compare to a standard ensemble of $M^N$ separate models, each one trained on a subset of data (this is not practical at scale, but would highlight the upper bound if each expert path was like a separate model). Obviously, $M^N$ grows huge, so this is only conceptual except maybe for small N.

**Model Configuration:** For each architecture (EoE and baselines), we would ensure the total number of parameters and training budget are as equal as possible for a fair comparison. EoE introduces hyperparameters $M$ (experts per layer), $N$ (number of layers), and connectivity pattern. We might experiment with different $N$ (e.g. 2-layer vs 3-layer EoE) to see if deeper expert hierarchies help. Connectivity can be set as full (each expert connects to all next-layer experts, meaning gating decisions are conditionally independent except for the input representation) or partial (structured connectivity). A simple default is full connectivity (any expert in layer $\ell$ can route to any expert in $\ell+1$) and let gating learn the conditional pathways. More structured connectivity (like forming a tree) could be experimented with to see if it improves specialization or stability.

**Training Protocol:** All models would be trained from scratch on the tasks. We would use similar optimization settings (e.g. Adam optimizer, similar learning rates, number of training steps) as used in prior MoE and AoE experiments ([Autonomy-of-Experts Models](https://arxiv.org/html/2501.13074v1#:~:text=are%20ranked%20based%20on%20their,MoE%20models%20with%20comparable%20efficiency))  For EoE, additional care in training might include:
- A **load balancing loss** term at each layer’s router to prevent collapse (ensuring each expert gets roughly equal share of routing). This was critical in MoE models like Switch Transformer ([Autonomy-of-Experts Models](https://arxiv.org/html/2501.13074v1#:~:text=experts%C2%A0%28Shazeer%20et%C2%A0al,2021%3B%20Clark%20et%C2%A0al)) 
- Possibly curriculum or two-phase training: first train gating networks with simpler experts or fewer layers, then gradually increase expert specialization.
- Monitoring of expert utilization: we would track how often each expert is chosen, at each layer, to ensure the model is using all of them and not, say, ignoring an entire layer or subset.

**Evaluation Metrics:** We will evaluate models on both **performance metrics** and **efficiency metrics**:
- *Performance:* For language tasks, **perplexity** (for language modeling) or **BLEU** score (for translation) will be used. For classification tasks, **accuracy** or **Top-1/Top-5 accuracy** is the metric. For QA, **accuracy** or **F1** if applicable. We will also measure generalization (performance on validation/test sets, possibly across different domains to test if experts capture domain-specific knowledge).
- *Efficiency:* We will measure the **inference FLOPs** or estimated operations per input for each model – expecting EoE to use fewer FLOPs than an equivalent dense model and comparable to a flat MoE. We will also look at **inference latency** (time per sample) and **throughput** (samples per second) on a given hardware setup, to quantify the overhead (if any) introduced by sequential expert layers.
- *Capacity usage:* As a diagnostic, measure how many experts (and layers) are active per input on average. This helps confirm the sparsity (e.g. EoE might only activate, say, 3 out of 3×M experts total per input, which is 1 per layer).
- *Quality of specialization:* While harder to quantify, we can devise metrics: for example, cluster inputs by which expert handled them and see if those clusters correspond to intuitive categories (using mutual information with known categories or silhouette scores in representation space). If doing multimodal, check that images mostly go to image experts, etc.
- *Robustness:* It could be insightful to test model robustness to distribution shift: if one expert fails or is removed, does the model adapt by using alternate experts? Because EoE has multiple layers, a failure in one expert could potentially be compensated by gating choosing a different path. We can simulate an expert “dropping out” and see if performance gracefully degrades or if it catastrophically fails (which would indicate too much reliance on a single expert).

The experimental results will allow us to compare EoE to prior architectures in terms of both raw performance and practical efficiency. We expect EoE to particularly shine in scenarios with heterogeneous data (multi-domain or multimodal), where its layered routing can adapt, and under constraints where sparsity is crucial (large models that need to be efficient). 

## Feasibility and Challenges 
Implementing and scaling the EoE architecture comes with several practical challenges. In this section, we discuss the feasibility of realizing EoE in real-world systems and outline potential solutions and optimizations to address these challenges.

**Model Implementation Complexity:** EoE introduces multiple routing stages within a single model. Modern deep learning frameworks can handle conditional computation (e.g., using masking or indexing to select experts), but ensuring this runs efficiently on hardware is non-trivial. One challenge is that different inputs in a batch might take different expert paths, which leads to irregular computation patterns (some inputs use expert $A$, others use expert $B$, etc.). Efficiently batching these computations requires grouping inputs by the expert they go to at each layer. Libraries for MoE, such as the one used in Switch Transformer, already implement mechanisms to route tokens to experts and then combine outputs. These can be extended hierarchically: for layer 1, route tokens to experts; then for layer 2, route each token to an expert based on its layer1 result. This likely requires a custom kernel or recursive routing function. Ensuring memory locality and minimizing data transfer between experts layers is key. A possible optimization is **pipeline parallelism** across layers: while one batch of tokens is in layer2, the next batch can be entering layer1, etc., to utilize all expert layers in parallel (important in distributed setups).

**Scalability and Distributed Training:** In large-scale setups, experts are often placed on different devices (e.g., each expert on a different GPU, or groups of experts per GPU). In an EoE, one might distribute by layers (layer1 experts on GPU1, layer2 on GPU2, etc.) or by experts (round-robin experts across many GPUs). Distributing by layer could simplify the routing (all outputs from layer1’s device go to layer2’s device), but it might create a *bottleneck* if one layer is slower or has more load. Distributing experts within each layer across devices (as done in flat MoE) would require routing traffic both horizontally (among experts in same layer across devices) and vertically (to the next layer’s devices). This increases communication complexity. However, since only top-$k$ experts are chosen, the communication volume is limited: we only need to send data to at most $k$ experts’ devices per layer per input. Techniques like *load balancing* help ensure one device isn’t overloaded. Another challenge is the *state synchronization* – if using AoE gating, each expert needs the input to compute its activation; that might mean broadcasting the input to all experts in a layer, which is expensive. AoE authors solved this by a low-rank factorization trick to reduce communication ([Autonomy-of-Experts Models](https://arxiv.org/html/2501.13074v1#:~:text=awareness%20reflected%20in%20the%20scale,MoE%20models%20with%20comparable%20efficiency))  For EoE, similar strategies can be applied at each layer: compress the input representation, broadcast it to experts to compute gating signals, then proceed with top experts.

**Training Stability:** MoE models are known to suffer from unstable training if not properly regularized (experts collapsing or gating oscillating). EoE adds more gating layers, which could compound this instability. Each gating decision is a non-differentiable choice if done hard; in practice we use soft gating (continuous probabilities) during training and sometimes even a noisy top-$k$ selection to keep things differentiable. We will likely need to use **auxiliary loss terms** at each layer’s gating to encourage a smooth distribution of assignments (e.g., an entropy term to avoid extremely hard decisions too early in training, or a load-balance term as mentioned). Another challenge is that errors can propagate: if an early layer misroutes, the later layers won’t get good gradient signal for that input. One mitigation is to allow some fraction of traffic to be “exploratory.” For example, occasionally route inputs to two different experts and split the computation (this is like dropout for experts). This could help the model discover if an alternate routing would reduce loss, and adjust gating accordingly. Such techniques need careful tuning to not blow up computation.

**Expert Optimization and Skip Connections:** We can incorporate known optimization tricks. For instance, **expert capacity**: in many MoE implementations, each expert has a fixed capacity (number of tokens it can process at once) to prevent one expert from taking too many inputs. In EoE, since each layer’s gating is narrower in focus, we might be able to increase per-expert capacity without collision, but we should still be cautious of any one expert getting overwhelmed. If we encounter an expert saturated, it indicates gating or load imbalance issues. Another idea is **skipping entire layers** for certain inputs: if an input is very simple or confident, maybe layer1’s expert can directly produce an output without needing deeper experts. While our formalization assumes the input goes through all $N$ layers, one could imagine a dynamic mechanism where an expert at layer $\ell$ can decide “this is enough, output now.” This is analogous to an adaptive computation time (ACT) mechanism or early exiting in deep networks. Implementing this would complicate things but could yield efficiency gains by not always using full depth. EoE in principle could be extended to have a stopping criterion at each layer (another bit of meta-expert decision). We do not emphasize this in the current design, but it’s a potential future optimization.

**Memory Footprint:** With many experts, memory usage (for parameters) is high. However, one benefit of MoE-style sparsity is at runtime you only need to load the active experts’ weights for computation. Frameworks can unload or swap out inactive experts if memory is constrained (though this may hurt speed). Techniques like quantization or using smaller experts can also manage footprint. Training all experts simultaneously is heavy – one can consider expert-specific schedules or periodic freezing (similar to Mixture-of-Experts training in some meta-learning settings, where not all experts are updated each step). This could reduce memory pressure by not having to store huge optimizer states for rarely-used experts all the time. 

**Debugging and Monitoring:** EoE’s complexity means debugging is important. We need robust monitoring of the routing decisions during training. Visualization tools can be developed to show the “traffic” through the expert grid. If some experts are never used, one might consider removing or re-initializing them to something else. If one path is dominating, perhaps the gating temperature can be adjusted. Essentially, training an EoE might involve more hyperparameter tuning (gating network architecture, temperature, loss weights) than a standard model, as we have multiple layers of those to tune.

**Scalability in Terms of $N$ and $M$:** How far can we push the grid size? If $N$ is too large (say 10 layers), the model becomes very deep in terms of routing decisions – this could be hard to train (vanishing gradient through many softmax gates) and likely unnecessary for most tasks. If $M$ is extremely large (hundreds of experts per layer), then even computing gating scores for all of them is heavy, although approaches like hashing-based gating or two-level gating (coarse pre-gate to shortlist experts) could help. In practice, current MoE models use up to a few thousand total experts in a single layer ([[1701.06538] Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer](https://arxiv.org/abs/1701.06538#:~:text=improvements%20in%20model%20capacity%20with,combination%20of%20these%20experts%20to))  EoE might instead use, say, 32 experts per layer but 4 layers, totaling 128 experts distributed – which might be more manageable at each layer. We expect diminishing returns beyond a certain number of experts or layers, and those would be discovered empirically.

Despite these challenges, none are insurmountable. The recent progress with large MoEs (which faced similar issues of load balancing and distributed training) provides a roadmap. By leveraging existing techniques (expert dropout, load balancing loss, efficient implementations) and introducing new ones (hierarchical load balancing, adaptive routing), EoE can be implemented at scale. The potential payoff – a model that is both highly **scalable** and **specialized** – justifies tackling these engineering challenges.

## Conclusion 
We have introduced the **Ensemble of Experts (EoE)** architecture, a new extension of mixture-of-experts models that organizes experts in a multi-layer hierarchical ensemble. EoE is motivated by analogies to layered decision-making in neural networks and human cognition, and it generalizes previous expert architectures like Jordan’s HME and Ahn’s NMoE by allowing an $M \times N$ grid of experts with conditional routing between layers. We formally defined the EoE structure and discussed how each layer’s experts and gating build upon the previous, culminating in a final output produced by specialized final-layer experts. Theoretical analysis suggests that EoE can achieve greater specialization and efficient use of parameters, thanks to its sparse, stage-wise activation of experts. Key advantages include **sparse computation** at scale, **coarse-to-fine specialization** of knowledge, and the ability to naturally handle heterogeneous inputs by routing them through appropriate expert paths.

Our proposed experimental framework will benchmark EoE against state-of-the-art MoE variants (including sparsely-gated MoE and AoE) on tasks in NLP, vision, and beyond. We hypothesize that EoE will particularly excel in multi-domain or multimodal scenarios, where hierarchical routing can partition the problem better than a one-size-fits-all layer. We also expect gains in sample efficiency or generalization, as EoE’s experts can capture niche patterns that a flat model might miss. The experiments will illuminate how EoE’s benefits trade off against added complexity.

We have also acknowledged the **feasibility considerations** and outlined solutions for implementing EoE at scale. While training a multi-layer expert model is undoubtedly more involved than standard architectures, the path has been paved by prior MoE research in terms of software and algorithmic techniques. With careful design of the gating mechanism (potentially leveraging AoE’s autonomous routing at each layer) and ensuring balanced expert utilization, an EoE can be trained to convergence on large data. Scalability tests will inform the practical limits of the approach, and may lead to improved variants (for example, an EoE with adaptive depth, or hybrid EoE-MoE structures).

**Potential Applications:** Beyond the benchmarks, EoE opens up many avenues for real-world applications. In large-scale **multitask learning**, an EoE model could allocate different expert pathways to different tasks (e.g., translation vs. summarization vs. Q&A), allowing a single network to handle all tasks efficiently – essentially a neural *app store* where each expert is an app and the router selects which to run. In **continual learning**, one could add new experts to a layer for new tasks or domains without affecting existing ones, since the gating can be trained to route new domain inputs to the new expert (preserving old expertise). In **reinforcement learning** or **robotics**, an EoE policy network could switch between different controllers based on the state (analogous to a hierarchical finite-state controller). Moreover, the interpretability of EoE’s routing could be useful in sensitive applications: one could verify that certain inputs always go through a safe “sanity-check” expert, etc. 

**Future Research Directions:** This work just scratches the surface of what is possible with ensembles of experts. Some intriguing directions include:
- **Learning Connectivity:** In our formulation, the connectivity (which expert leads to which next experts) could be learned rather than fixed. This would be akin to a neural architecture search over the expert graph. One could imagine using gradient-based methods or even evolutionary strategies to evolve the expert network topology for optimal performance.
- **Dynamic Expert Addition/Removal:** During training, we might add experts if existing ones become saturated or if a new cluster of data emerges. Conversely, merge or remove experts that are redundant. This keeps the ensemble efficient over time.
- **AoE + EoE Synergy:** We proposed EoE as an extension of both MoE and AoE. A fully autonomous EoE would let each expert in each layer decide on input routing. This might require a decentralized consensus mechanism (since multiple experts at layer $\ell$ might all think they should route the input to layer $\ell+1$). Developing an algorithm for multi-layer AoE (experts voting not just to execute but also perhaps indicating which next expert should take over) would be complex but could remove gating networks entirely.
- **Theory of Expert Depth:** From a theoretical perspective, it would be valuable to understand under what conditions a deep expert hierarchy is provably more expressive or easier to learn than a flat one. This could connect to decision tree theory or to the theory of function composition. Perhaps certain functions are exponentially easier to represent with $N$ layers of experts than with 1 layer.
- **Combining with Dense Layers:** Not every part of a model needs to be sparse experts. EoE could be used in conjunction with regular dense layers. For example, in a Transformer, one might alternate dense self-attention layers with EoE feed-forward layers. Investigating such hybrids could yield the best of both worlds (dense layers for general features, expert layers for specialized knowledge).

In conclusion, the Ensemble of Experts architecture offers a powerful framework to scale up neural networks intelligently. By leveraging a structured ensemble of specialized components, it aims to mimic a team-of-experts strategy within a single model. We have detailed the design, related it to its predecessors, and provided initial guidance on implementation and evaluation. We believe EoE has the potential to advance the frontier of efficient deep learning, enabling models that are not just larger, but *smarter* in how they utilize their myriad components. Future work will tell how far this idea can go, and we hope this paper lays a solid foundation for exploring the vast design space of expert ensembles. 

